{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOoeRfDzwfV/kRks/6MVpWk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MNIST + PyTorch\n","\n","**MNIST dataset** je jedním z nejstarších klasických datasetů v oblasti umělé inteligence.\n","\n","**PyTorch** je nástroj pro vytváření a trénování neuronových sítí.\n","\n","**Tenzory** jsou vícerozměrné \"matice\": objekty plné čísel, které lze indexovat ve více dimenzích. Vektory i matice jsou také tenzory, jednorozměrné a dvourozměrné. Tenzory jsou základní stavební kámen umělé inteligence."],"metadata":{"id":"7pbwQFKlcrRd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F"],"metadata":{"id":"qrg5pUmdxmyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Seznámení se s datasetem"],"metadata":{"id":"8Jg0sm2Ay_Jt"}},{"cell_type":"code","source":["transform = transforms.ToTensor()\n","dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)"],"metadata":{"id":"paYAWs1jxr0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y = dataset[0]"],"metadata":{"id":"yXNxjVv5yOYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"hAFBC31lyhOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.type()"],"metadata":{"id":"f3IE8zFOe-AP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"RyXATRZzyh2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k = random.randint(0, 59_999)\n","x, y = dataset[k]\n","for i in range(28):\n","  for j in range(28):\n","    s = ' ' if x[0, i, j] < 0.2 else ('.' if x[0, i, j] < 0.7 else 'x')\n","    print(s, end='')\n","  print()\n","print(f'Target = {y}')"],"metadata":{"id":"SIM_HXxtyi9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows, cols = 3, 4\n","fig, axes = plt.subplots(rows, cols, figsize=(8, 5))\n","for i, ax in enumerate(axes.flat):\n","  x, y = dataset[i]\n","  ax.imshow(x[0], cmap='gray')\n","  ax.set_title(y)\n","  ax.axis('off')\n","plt.tight_layout()"],"metadata":{"id":"QDPmTKvfyl0W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## K zamyšlení\n","- Pokud byste měli počítač naučit, jak rozpoznávat tyto cifry, jak byste to vůbec dělali?\n","- A když ho to naučíte, jakým způsobem to můžeme ohodnotit? Abychom mohli říct, jestli se to naučil dobře nebo ne?"],"metadata":{"id":"kmCcrhRDzReQ"}},{"cell_type":"markdown","source":["## Neuronová síť"],"metadata":{"id":"NHBXpQA0zHW1"}},{"cell_type":"code","source":["# Structure of the network\n","n_input = 28 * 28   # number of neurons in the input layer (~ number of pixels of images)\n","n_hidden_1 = 16     # number of neurons in the first hidden layer\n","n_hidden_2 = 16     # number of neurons in the second hidden layer\n","n_output = 10       # number of neurons in the output layer (~ number of different digits)"],"metadata":{"id":"2R3J2N-qu8tI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform,\n","                               download=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transform,\n","                              download=True)\n","\n","# Use tensor dataset instead - much faster\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","train_tensor_dataset = TensorDataset((train_dataset.data / 255.).to(device), train_dataset.targets.to(device))\n","test_tensor_dataset = TensorDataset((test_dataset.data / 255.).to(device), test_dataset.targets.to(device))\n","\n","# Create data loaders\n","batch_size = 64     # size of batch\n","train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_tensor_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"teYJkHxFy0Hw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Definice modelu"],"metadata":{"id":"98BWQSXmh9Va"}},{"cell_type":"code","source":["# Definition of the neural network\n","class SmallNetwork(nn.Module):\n","  def __init__(self):\n","    super(SmallNetwork, self).__init__()\n","    self.fc1 = nn.Linear(28 * 28, 16)\n","    self.fc2 = nn.Linear(16, 16)\n","    self.fc3 = nn.Linear(16, 10)\n","\n","  def forward(self, x):\n","    x = x.view(x.size(0), -1)\n","    x = torch.sigmoid(self.fc1(x))\n","    x = torch.sigmoid(self.fc2(x))\n","    x = self.fc3(x)\n","    return x"],"metadata":{"id":"8jZ2cIX4ZBk7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sigmoid = activation function\n","x = torch.linspace(-6, 6, 201)\n","plt.plot(x, torch.sigmoid(x))\n","plt.grid()"],"metadata":{"id":"lmpLX8DyZvZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SmallNetwork()"],"metadata":{"id":"L6Ey7-snbabi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y = dataset[0]"],"metadata":{"id":"cSFr8p7eb4f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(x)"],"metadata":{"id":"SedXNJa3b834"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.argmax(model(x))"],"metadata":{"id":"-sXdUw2DcO_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_prediction(dataset, model, rows=3, cols=4, random=False):\n","  fig, axes = plt.subplots(rows, cols, figsize=(8, 5))\n","  for i, ax in enumerate(axes.flat):\n","    if random:\n","      i = np.random.randint(len(dataset))\n","    x, y = dataset[i]\n","    pred = torch.argmax(model(x))\n","    ax.imshow(x[0], cmap='gray')\n","    ax.set_title(f'label = {y}, pred = {pred}')\n","    ax.axis('off')\n","  plt.tight_layout()"],"metadata":{"id":"XWYUHqXwbaJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_prediction(dataset, model, random=True)"],"metadata":{"id":"4pYWE5Iic_i_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prediction_table(dataset, model):\n","  _, predicted = torch.max(model(dataset.data.float() / 255.0), 1)\n","  df = pd.DataFrame({'True': dataset.targets.cpu(),\n","                     'Prediction': predicted.cpu()}, dtype=int)\n","  df['Count'] = 1\n","  pivoted = df.groupby(['True', 'Prediction'])['Count'].count().reset_index() \\\n","    .pivot_table(index='True', columns='Prediction', values='Count', fill_value=0)\n","  for i in range(10):\n","    if i not in pivoted.columns:\n","      pivoted[i] = 0\n","  return pivoted.sort_index(axis=1)"],"metadata":{"id":"l3MBwOZLbaAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_table(test_dataset, model)"],"metadata":{"id":"gfhPh00UbZ4y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Trénink"],"metadata":{"id":"Av0XLqlPbY96"}},{"cell_type":"code","source":["def train_network(model, train_loader, test_loader, num_epochs=20):\n","  cross_entropy_loss = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","  train_losses = []\n","  test_losses = []\n","  train_accuracies = []\n","  test_accuracies = []\n","\n","  # Loss and accuracy before the training.\n","  model.eval()\n","  test_loss = 0.0\n","  test_correct = 0\n","  with torch.no_grad():\n","    for images, labels in test_loader:\n","      outputs = model(images)\n","      loss = cross_entropy_loss(outputs, labels)\n","      test_loss += loss.item() * images.size(0)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_correct += (predicted == labels).sum().item()\n","\n","  test_loss = test_loss / len(test_loader.dataset)\n","  test_losses.append(test_loss)\n","  test_accuracy = test_correct / len(test_loader.dataset)\n","  test_accuracies.append(test_accuracy)\n","\n","  train_losses.append(np.nan)\n","  train_accuracies.append(np.nan)\n","\n","  t0 = datetime.now()\n","  print(f\"Starting training at {t0} on device {next(model.parameters()).device}.\"\n","        f\" Test loss {test_loss:.4f}, accuracy {100 * test_accuracy:.2g}%.\")\n","  print()\n","\n","  for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    train_correct = 0\n","    model.train(True)\n","    for images, labels in train_loader:\n","      outputs = model(images)\n","      loss = cross_entropy_loss(outputs, labels)\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      train_loss += loss.item() * images.size(0)\n","      _, predicted = torch.max(outputs.data, 1)\n","      train_correct += (predicted == labels).sum().item()\n","\n","    model.eval()\n","    test_loss = 0.0\n","    test_correct = 0\n","    with torch.no_grad():\n","      for images, labels in test_loader:\n","        outputs = model(images)\n","        loss = cross_entropy_loss(outputs, labels)\n","        test_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        test_correct += (predicted == labels).sum().item()\n","\n","    train_loss = train_loss / len(train_loader.dataset)\n","    train_losses.append(train_loss)\n","    train_accuracy = train_correct / len(train_loader.dataset)\n","    train_accuracies.append(train_accuracy)\n","\n","    test_loss = test_loss / len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","    test_accuracy = test_correct / len(test_loader.dataset)\n","    test_accuracies.append(test_accuracy)\n","\n","    print(f\"Elapsed {datetime.now() - t0}s, epoch {epoch + 1} / {num_epochs}.\",\n","          f\"Training loss {train_loss:.4f}, accuracy {100 * train_accuracy:.1f}%.\",\n","          f\"Test loss {test_loss:.4f}, accuracy {100 * test_accuracy:.1f}%.\")\n","\n","  print()\n","  print(f\"Finished in {datetime.now() - t0}s.\")\n","  return train_losses, train_accuracies, test_losses, test_accuracies"],"metadata":{"id":"WTxzbLj7ZrFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss, train_acc, test_loss, test_acc = train_network(\n","    model, train_loader, test_loader, num_epochs=150)"],"metadata":{"id":"EFF0e3bGa76I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Výsledky"],"metadata":{"id":"UOJLqXL6t93X"}},{"cell_type":"code","source":["plt.plot(train_loss, 'g', label='Train loss')\n","plt.plot(test_loss, 'r', label='Test loss')\n","plt.title('Loss')\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"DhMGFe0ziTAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_acc, 'g', label='Train accuracy')\n","plt.plot(test_acc, 'r', label='Test accuracy')\n","plt.title('Accuracy')\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"TFNX1osTrBRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_prediction(test_dataset, model)"],"metadata":{"id":"aTmi7oalqHds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_table(test_dataset, model)"],"metadata":{"id":"Fh0v6kjlpCZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Matematický bonus"],"metadata":{"id":"ipHThAn_uBCO"}},{"cell_type":"markdown","source":["Celá neuronová síť je jen maticové násobení, podobně jako v našem příkladu na papíře. Stačí k tomu přidat aktivační funkci a biases a síť učit pomocí derivací."],"metadata":{"id":"7sxFauRfuQqd"}},{"cell_type":"code","source":["# initialization - this is identical neural network as the one above\n","def initialize_params():\n","  return {'W1': np.random.randn(n_hidden_1, n_input) * np.sqrt(1 / n_input),\n","          'b1': np.zeros((n_hidden_1, 1)),\n","          'W2': np.random.randn(n_hidden_2, n_hidden_1) * np.sqrt(1. / n_hidden_1),\n","          'b2': np.zeros((n_hidden_2, 1)),\n","          'W3': np.random.randn(n_output, n_hidden_2) * np.sqrt(1. / n_hidden_2),\n","          'b3': np.zeros((n_output, 1))}"],"metadata":{"id":"mgCHHrDituTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# And we need to define some helper function:\n","\n","# sigmoid\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","# one-hot encoding\n","def one_hot(arr):\n","    res = np.zeros((arr.size, arr.max() + 1))\n","    res[np.arange(arr.size), arr] = 1\n","    return res\n","\n","# softmax (is really a soft version of maximum)\n","def softmax(z):\n","    return np.exp(z) / np.sum(np.exp(z), axis=0)\n","\n","# loss function - the cost we try to minimize\n","def loss_mean_squared_errors(Y, Y_hat):\n","    return np.sum((Y - Y_hat) ** 2) / Y.shape[1]\n","\n","def accuracy(Y, Y_hat):\n","    Y_idx = np.argmax(Y, axis=0)\n","    Y_hat_idx = np.argmax(Y_hat, axis=0)\n","    return np.sum(Y_idx == Y_hat_idx) / len(Y_idx)"],"metadata":{"id":"KAWYybZ0wDGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feed_forward(X, params):\n","    \"\"\"\n","    feed forward:\n","    inputs: params: a dictionary contains all the weights and biases\n","    return: cache: a dictionary contains all the fully connected units and activations\n","    \"\"\"\n","    cache = {}\n","\n","    # Z1 = W1.dot(x) + b1\n","    cache['Z1'] = params['W1'] @ X + params['b1']\n","    # A1 = sigmoid(Z1)\n","    cache['A1'] = sigmoid(cache['Z1'])\n","\n","    # Z2 = W2.dot(A1) + b2\n","    cache['Z2'] = params['W2'] @ cache['A1'] + params['b2']\n","    # A2 = sigmoid(Z2)\n","    cache['A2'] = sigmoid(cache['Z2'])\n","\n","    # Z3 = W3.dot(A2) + b3\n","    cache['Z3'] = params['W3'] @ cache['A2'] + params['b3']\n","    # A3 = sigmoid(Z3)\n","    # cache['A3'] = softmax(cache['Z3'])\n","    cache['A3'] = sigmoid(cache['Z3'])\n","\n","    return cache"],"metadata":{"id":"VQmy5NChvywa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def back_propagate(X, Y, params, cache, m_batch):\n","    \"\"\"\n","    back propagation\n","    inputs:\n","        params: a dictionary contains all the weights and biases\n","        cache: a dictionary contains all the fully connected units and activations\n","    return:\n","        grads: a dictionary contains the gradients of corresponding weights and biases\n","    \"\"\"\n","    # error at last layer - mean square errors\n","    dA3 = 2 * (cache['A3'] - Y)\n","    dZ3 = dA3 * cache['A3'] * (1 - cache['A3'])\n","\n","    # or softmax with cross-entropy\n","    # dZ3 = cache['A3'] - Y\n","\n","    # gradients at last layer\n","    dW3 = (1 / m_batch) * np.matmul(dZ3, cache['A2'].T)\n","    db3 = (1 / m_batch) * np.sum(dZ3, axis=1, keepdims=True)\n","\n","    # back propagate through the second layer\n","    dA2 = np.matmul(params['W3'].T, dZ3)\n","    dZ2 = dA2 * cache['A2'] * (1 - cache['A2'])\n","\n","    # gradients the second last layer\n","    dW2 = (1. / m_batch) * np.matmul(dZ2, cache['A1'].T)\n","    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n","\n","    # back propagate through the first layer\n","    dA1 = np.matmul(params['W2'].T, dZ2)\n","    dZ1 = dA1 * cache['A1'] * (1 - cache['A1'])\n","\n","    # gradients at the first layer\n","    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n","    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n","\n","    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3}\n","\n","    return grads"],"metadata":{"id":"9pvlzZVowEYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_params(params, X_train, Y_train, X_test, Y_test, num_epochs=20, lr=1):\n","\n","  train_losses = []\n","  test_losses = []\n","\n","  train_accuracies = []\n","  test_accuracies = []\n","\n","  t0 = datetime.now()\n","  print(f\"Starting manual training at {t0}.\")\n","  print()\n","\n","  for i in range(num_epochs):\n","\n","      # shuffle training set\n","      permutation = np.random.permutation(X_train.shape[1])\n","      X_train_shuffled = X_train[:, permutation]\n","      Y_train_shuffled = Y_train[:, permutation]\n","\n","      batches = (X_train_shuffled.shape[0] - 1) // batch_size + 1\n","\n","      for j in range(batches):\n","\n","          # get mini-batch\n","          begin = j * batch_size\n","          end = min(begin + batch_size, X_train.shape[1] - 1)\n","          X = X_train_shuffled[:, begin:end]\n","          Y = Y_train_shuffled[:, begin:end]\n","          m_batch = end - begin\n","\n","          # forward and backward\n","          cache = feed_forward(X, params)\n","          grads = back_propagate(X, Y, params, cache, m_batch)\n","\n","          # gradient descent\n","          params['W1'] = params['W1'] - lr * grads['dW1']\n","          params['b1'] = params['b1'] - lr * grads['db1']\n","          params['W2'] = params['W2'] - lr * grads['dW2']\n","          params['b2'] = params['b2'] - lr * grads['db2']\n","          params['W3'] = params['W3'] - lr * grads['dW3']\n","          params['b3'] = params['b3'] - lr * grads['db3']\n","\n","      # forward pass on training set\n","      cache = feed_forward(X_train, params)\n","      train_loss = loss_mean_squared_errors(Y_train, cache['A3'])\n","      train_losses.append(train_loss)\n","      train_accuracy = accuracy(Y_train, cache['A3'])\n","      train_accuracies.append(train_accuracy)\n","\n","      # forward pass on test set\n","      cache = feed_forward(X_test, params)\n","      test_loss = loss_mean_squared_errors(Y_test, cache['A3'])\n","      test_losses.append(test_loss)\n","      test_accuracy = accuracy(Y_test, cache['A3'])\n","      test_accuracies.append(test_accuracy)\n","\n","      print(f\"Elapsed {datetime.now() - t0}s, epoch {i + 1} / {num_epochs}.\",\n","            f\"Training loss {train_loss:.4f}, accuracy {100 * train_accuracy:.1f}%.\",\n","            f\"Test loss {test_loss:.4f}, accuracy {100 * test_accuracy:.1f}%.\")\n","\n","  print()\n","  print(f\"Finished in {datetime.now() - t0}s.\")\n","  return train_losses, train_accuracies, test_losses, test_accuracies"],"metadata":{"id":"tq_A563zIRuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, Y_train = (t.numpy() for t in train_tensor_dataset.tensors)\n","X_test, Y_test = (t.numpy() for t in test_tensor_dataset.tensors)\n","\n","X_train = X_train.reshape(-1, 28 * 28).T\n","X_test = X_test.reshape(-1, 28 * 28).T\n","\n","Y_train = one_hot(Y_train).T\n","Y_test = one_hot(Y_test).T"],"metadata":{"id":"ZIhb-_xAJ3sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = initialize_params()"],"metadata":{"id":"ZrzFsBzhQOyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss, train_acc, test_loss, test_acc = train_params(\n","    params, X_train, Y_train, X_test, Y_test, num_epochs=250)"],"metadata":{"id":"imZqcXb9KQp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_loss, 'g', label='Train loss')\n","plt.plot(test_loss, 'r', label='Test loss')\n","plt.title('Loss')\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"D9LUJ59NPCZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_acc, 'g', label='Train accuracy')\n","plt.plot(test_acc, 'r', label='Test accuracy')\n","plt.title('Accuracy')\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"_8ELvcT7PkWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params"],"metadata":{"id":"VRr64FTrP_9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_test = np.argmax(feed_forward(X_test, params)['A3'], axis=0)\n","fig, axes = plt.subplots(rows, cols, figsize=(8, 5))\n","for i, ax in enumerate(axes.flat):\n","  if random:\n","    i = np.random.randint(X_test.shape[1])\n","  x, y = X_test[:, i], Y_test[:, i]\n","  pred = pred_test[i]\n","  ax.imshow(x.reshape(28, 28), cmap='gray')\n","  ax.set_title(f'label = {np.argmax(y)}, pred = {pred}')\n","  ax.axis('off')\n","plt.tight_layout()"],"metadata":{"id":"O1hTPc-mQAkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# what the network is actually predicting?\n","pred_test = np.argmax(feed_forward(X_test, params)['A3'], axis=0)\n","true_test = np.argmax(Y_test, axis=0)\n","\n","df = pd.DataFrame({'True': true_test, 'Prediction': pred_test})\n","df['Count'] = 1\n","pivoted = df.groupby(['True', 'Prediction'])['Count'].count().reset_index() \\\n","    .pivot_table(index='True', columns='Prediction', values='Count', fill_value=0)\n","for i in range(10):\n","    if i not in pivoted.columns:\n","        pivoted[i] = 0\n","pivoted = pivoted[range(10)]\n","pivoted"],"metadata":{"id":"wfWZK1kaRA8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KEK7S4oURQpE"},"execution_count":null,"outputs":[]}]}